<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-01-04T21:17:34-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">alpha</title><subtitle>Posting cool proofs worth remembering </subtitle><entry><title type="html">Expanding $e^x$ without calculus</title><link href="http://localhost:4000/euler's/number/2025/01/04/Lessons.html" rel="alternate" type="text/html" title="Expanding $e^x$ without calculus" /><published>2025-01-04T00:00:00-05:00</published><updated>2025-01-04T00:00:00-05:00</updated><id>http://localhost:4000/euler&apos;s/number/2025/01/04/Lessons</id><content type="html" xml:base="http://localhost:4000/euler&apos;s/number/2025/01/04/Lessons.html"><![CDATA[<p>2025 ðŸŽ†</p>

<p>A quick proof of the taylor series of $e^x$ without using derivatives.</p>

\[e^x = \lim_{n \to \infty} \left(1+\frac{x}{n}\right)^{n} \\
\ \\
\text{Using binomial expansion:} \\
\ \\
e^x = \lim_{n \to \infty} \sum_{k=0}^{n} \binom{n}{k} \cdot \frac{x^k}{n^k}  \\
\ \\
= \lim_{n \to \infty} \sum_{k=0}^{n} \frac{n(n-1) \ldots (n-k+1)}{k!} \cdot \frac{x^k}{n^k} \\
\ \\
= \sum_{k=0}^{n} \frac{x^k}{k!} \cdot \underbrace{\lim_{n \to \infty} \frac{n(n-1) \ldots (n-k+1)}{n^k}}_{=1} \\
\ \\
= \sum_{k=0}^{n} \frac{x^k}{k!}\]]]></content><author><name></name></author><category term="Euler&apos;s" /><category term="Number" /><summary type="html"><![CDATA[2025 ðŸŽ†]]></summary></entry><entry><title type="html">Cross Product Definition</title><link href="http://localhost:4000/linear/algebra/2024/12/21/Lessons.html" rel="alternate" type="text/html" title="Cross Product Definition" /><published>2024-12-21T00:00:00-05:00</published><updated>2024-12-21T00:00:00-05:00</updated><id>http://localhost:4000/linear/algebra/2024/12/21/Lessons</id><content type="html" xml:base="http://localhost:4000/linear/algebra/2024/12/21/Lessons.html"><![CDATA[<h2 id="the-cross-product">The cross product</h2>

<p>The cross product $\times$ is one way to define the multiplication of vectors with a linear operator. Similar to the dot product, the cross product has its own geometric interpretation.</p>

<p>Geometrically, the cross product of $\vec{a}$ and $\vec{b}$ in 3D is defined as:</p>

<blockquote>
  <p>$\vec{a} \times \vec{b} = $ a certain vector $\vec{c}$ orthogonal to both vectors with the magnitude of the signed area of the parallelogram with sides $\vec{a}$ and $\vec{b}$ .</p>
</blockquote>

<p>However, when it comes to actually calculating $\vec{a} \times \vec{b}$ , the technique used seems utterly disconnected from the geometric definition.</p>

<blockquote>
  <p>$\vec{a} \times \vec{b} = 
\begin{vmatrix}
    \hat{i} &amp; \hat{j} &amp; \hat{k} \\<br />
    a_1 &amp; a_2 &amp; a_3 \\<br />
    b_1 &amp; b_2 &amp; b_3
\end{vmatrix}
= \begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} \hat{i} +
\begin{vmatrix} a_3 &amp; a_1 \\ b_3 &amp; b_1 \end{vmatrix} \hat{j} +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} \hat{k}
$</p>
</blockquote>

<h2 id="connecting-the-definitions">Connecting the definitions</h2>

<p>The presence of a determinant in the formula suggests a connection with geometry. After all, a determinant essentially represents the volume of a parallelepiped.</p>

<blockquote>
  <p>$ \begin{vmatrix} 
x_1 &amp; x_2 &amp; x_3 \\ y_1 &amp; y_2 &amp; y_3 \\ z_1 &amp; z_2 &amp; z_3 
\end{vmatrix} = $
volume of parallelepiped bounded by vectors $\vec{x}, \vec{y}, \vec{z}$</p>
</blockquote>

<p>The volume of the parallelepiped bounded by the three vectors  $\vec{a},\ \vec{b},$ and $\vec{c}= \vec{a} \times \vec{b}$ is equal to the area of the parallelogram bounded by $\vec{a}$ and $\vec{b}$ (which weâ€™ll call $A$) times the magnitude of $\vec{c}$. But since the magnitude of $\vec{c}$ equals $A$, the volume of the parallelepiped is equal to $A^2$.</p>

\[\begin{vmatrix} 
c_1 &amp; c_2 &amp; c_3 \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 
\end{vmatrix} =  A^2\]

<p>If we expand the determinant on the left, we find that</p>

\[\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} c_1 +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} c_2 +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} c_3
= A^2\]

<p>Now this is starting to look like our formula. Since $c_1, c_2, c_3$ are the x, y, and z components of $\vec{c}$ respectively, we can express each of those components as the dot product of $\vec{c}$ and a basis vector. This is a common technique to extract the components of a vector.</p>

\[\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} (\vec{c}\cdot\hat{i}) +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} (\vec{c}\cdot\hat{j}) +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} (\vec{c}\cdot\hat{k})
= A^2\]

<p>Since the dot product is a linear operator, we can distribute $\vec{c}$ out of the LHS.</p>

\[\vec{c} \cdot \left(
\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} \hat{i} +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} \hat{j} +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} \hat{k}
\right)
= A^2\]

<p>Now what about the RHS? Since $A$ is just the magnitude of $\vec{c}$, $A^2$ can be expressed as the dot product of $\vec{c}$ with itself.</p>

\[\vec{c} \cdot \left(
\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} \hat{i} +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} \hat{j} +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} \hat{k}
\right)
= \vec{c}\cdot\vec{c}\]

<p>There is only one vector $\vec{x} = \vec{c}$ that satisfies $\vec{c}\cdot\vec{x} = \vec{c}\cdot\vec{c}$. (This can be proven using a magnitude argument or by considering the equation $\vec{c}\cdot(\vec{x}-\vec{c}) = \vec{0}$ .) Therefore, we can see that</p>

\[\vec{c} \cdot \underbrace{\left(
\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} \hat{i} +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} \hat{j} +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} \hat{k}
\right)}_{\large{=\ \vec{c}}}
= \vec{c}\cdot\vec{c}\]

\[\Rightarrow
\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} \hat{i} +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} \hat{j} +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} \hat{k}
= \vec{c}\]

<p>This can be symbolically rewritten as</p>

\[\boxed{
\begin{vmatrix} a_2 &amp; a_3 \\ b_2 &amp; b_3 \end{vmatrix} \hat{i} +
\begin{vmatrix} a_3 &amp; a_1\\ b_3 &amp; b_1 \end{vmatrix} \hat{j} +
\begin{vmatrix} a_1 &amp; a_2 \\ b_1 &amp; b_2 \end{vmatrix} \hat{k}
= 
\begin{vmatrix} 
\hat{i} &amp; \hat{j} &amp; \hat{k} \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 
\end{vmatrix}
= \vec{c}
}\]

<p>And we have arrived at our original formula.</p>]]></content><author><name></name></author><category term="Linear" /><category term="Algebra" /><summary type="html"><![CDATA[The cross product]]></summary></entry></feed>